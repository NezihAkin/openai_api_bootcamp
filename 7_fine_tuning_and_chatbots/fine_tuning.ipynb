{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv(\"python_qa.csv\")\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.shape\n",
    "# I need body from this dataset as prompt and answer as the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = qa_df[\"Body\"]\n",
    "answers = qa_df[\"Answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to create json format that needed by OpenAI\n",
    "qa_openai_format = [{\"prompt\":q,\"completion\":a} for q,a in zip(questions,answers)]\n",
    "qa_openai_format[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part, before fine-tuning the model, it would be nice to count the tokens and calculate the cost. In that case, tiktoken library from openai gives the\n",
    "# token count in the way openai counts it.\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a func that returns the number of tokens from a string\n",
    "def num_tokens_from_string(string, enconding_name):\n",
    "    encoding = tiktoken.get_encoding(enconding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I will take the first 500 entries of my dataset to be used as an example for the fine tuning and create the necessary JSON file.\n",
    "import json\n",
    "dataset_size = 500\n",
    "\n",
    "with open(\"example_training_data.json\",\"w\") as f:\n",
    "    for entry in qa_openai_format[:500]:\n",
    "        f.write(json.dumps(entry))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's count the tokens and words in the example dataset and then calculate the cost for babbage model\n",
    "token_counter = 0\n",
    "word_counter = 0 \n",
    "\n",
    "for prompt_collection in qa_openai_format[:500]:\n",
    "    for prompt, completion in prompt_collection.items():\n",
    "        token_counter+= num_tokens_from_string(prompt, \"gpt2\")\n",
    "        token_counter+= num_tokens_from_string(completion, \"gpt2\")\n",
    "        word_counter+= len(prompt.split())\n",
    "        word_counter+= len(completion.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0006*4*token_counter/1000\n",
    "# Fine tuning with this dataset will cost around $0.47 or 47 cents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created this json file to do fine-tuning, openai recommends to use command line tool to perform it.\n",
    "\n",
    "Here are the commands for that:\n",
    "\n",
    "1. enter your openai API --> export OPENAI_API_KEY=<openai_api_key>\n",
    "2. openai api fine_tunes.create -t \"path_to_json_file\" -m <name_of_the_model_eg_babbage>\n",
    "3. It will take some time, if you still want to see the stream: openai api fine_tunes.follow -i <id_delivered>\n",
    "4. If you want to cancel fine-tuning: openai api fine_tunes.cancel -i <id_delivered>\n",
    "5. In the end, you will see the name of the model in the command line. You can call this model in Python IDE directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, now we can use our fine-tuned model\n",
    "fine_tuned_model = \"babbage:ft-personal-2023-03-08-15-54-05\" # I got this from the command line when fine tuning is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use it\n",
    "response = openai.Completion.create(\n",
    "    model = fine_tuned_model,\n",
    "    prompt = \"What are good Python books?\",\n",
    "    max_tokens = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_count = num_tokens_from_string(\"What are good Python books?\",\"gpt2\")\n",
    "prompt_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['choices'][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.0024/1000)*134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d4e298bcd666cea5634d864daa6b84a8070fd9981a6ce62f6b9d1dd5b6995d5"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
